services:
  api:
    build: ./backend
    container_name: api
    env_file:
      - .env
    command: sh -c "alembic upgrade head && uvicorn main:app --host 0.0.0.0 --port 8002"
    ports:
      - "8002:8002"
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped

  frontend:
    build: ./frontend
    container_name: frontend
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
    depends_on:
      - api
    restart: unless-stopped

  llm-model:
    image: vllm/vllm-openai:latest
    container_name: llm-model
    ports:
      - "8000:8000"
    volumes:
      - /mnt/storage/models/llm:/model
      - huggingface_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    command: >
      --model /model
      --gpu-memory-utilization 0.35
      --reasoning-parser qwen3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 10s
      retries: 10
    restart: unless-stopped

  llm-agent:
    build: ./LLM_backend
    container_name: llm-agent
    ports:
      - "8080:8080"
    environment:
      - VLLM_API_URL=http://llm-model:8000
      - VLLM_TIMEOUT=120
      - PYTHONUNBUFFERED=1
      - PATH="/home/llmuser/.local/bin:$PATH"
    depends_on:
      llm-model:
        condition: service_healthy
    restart: unless-stopped

  qdrant:
    build:
      context: .
      dockerfile_inline: |
        FROM qdrant/qdrant:latest
        RUN apt-get update -yq && apt-get install -yqq curl
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/healthz"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  ltm-api:
    build: ./LTM
    container_name: ltm-api
    ports:
      - "8006:8006"
    environment:
      - PYTHONUNBUFFERED=1
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - EMBEDDING_MODEL=intfloat/multilingual-e5-large
      - PATH="/home/ltmuser/.local/bin:$PATH"
    depends_on:
      qdrant:
        condition: service_healthy
    volumes:
      - logs:/app/logs
    restart: unless-stopped

  tg-bot:
    build: ./tg-bot
    container_name: tg-bot
    env_file:
      - .env
    ports:
      - "8004:8004"
    restart: unless-stopped

  image-gen:
    build: ./tbank_imagegen
    container_name: image-gen
    ports:
      - "8003:8003"
    volumes:
      - /mnt/storage/models/image-gen:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    restart: unless-stopped

  vlm:
    build: ./vlm
    container_name: vlm
    ports:
      - "8001:8001"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - /mnt/storage/models/vlm:/data
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    restart: unless-stopped

  postgres:
    image: postgres:17-alpine
    container_name: postgres
    env_file:
      - .env
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d $POSTGRES_DB -U $POSTGRES_USER"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: unless-stopped

volumes:
  postgres_data:
  huggingface_cache:
  logs:
  qdrant_storage:
